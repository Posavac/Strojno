{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b7fd06-7135-4826-aa59-b4ee5d71b020",
   "metadata": {},
   "source": [
    "# Vježbe 5 - drugi dio\n",
    "\n",
    "1. Overfitting i regularizacija\n",
    "    - Linearna regresija s regularizacijom\n",
    "    - Logistička regresija s regularizacijom\n",
    "    - Regularizacija u scikit-learn modelima\n",
    "    - Early stopping (DZ)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d1fe7f2-06d6-4155-8821-decd625ec756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c438231-fac2-415c-9d5d-0e486acdc50a",
   "metadata": {},
   "source": [
    "*Regularization is any modiﬁcation we make to a learning algorithm that is intended to reduce its generalization error but not it's training error.*\n",
    "[Goodfellow, Bengio, Courville - Deep Learning](https://www.deeplearningbook.org/)\n",
    "\n",
    "*Ako imate dvije teorije koje predviđaju isto, odaberite jednostavniju*. ([Occamova britva](https://hr.wikipedia.org/wiki/Occamova_britva))\n",
    "\n",
    "Ako model ima dovoljan broj parametara, nakon treniranja je moguća pojava **overfitanja**, gdje model izvrsno opisuje skup za treniranje, ali nema kvalitetnu generalizaciju, odnosno loše opisuje nove, neviđene podatke.\n",
    "\n",
    "Jedna metoda sprječavanja overfitanja je dodavanje regularizacijskog izraza funkciji cilja:\n",
    "$$ \n",
    "J(\\Theta) = J_{\\textrm{old}}(\\Theta) + \\lambda \\cdot R(\\Theta),\n",
    "$$\n",
    "\n",
    "gdje je $J_{\\textrm{old}}$ funkcija cilja koju smo ranije promatrali, $R(\\Theta)$ je regularizacijska funkcija, a $\\lambda$ je regularizacijski koeficijent koji kontrolira utjecaj regularizacijske funkcije $R(\\Theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97629e3-7d51-465e-98f7-cd95b6b7aab6",
   "metadata": {},
   "source": [
    "## Polinomna regresija s regularizacijom\n",
    "\n",
    "Jednostavni regularizacijski izraz s kojim smo se upoznali je $R(\\theta) = \\frac{1}{2}||\\Theta|| = \\frac{1}{2}\\Theta^T\\Theta$.\n",
    "\n",
    "U problemu linearne regresije funkcija cilja je prosječno kvadratno odstupanje te izraz s regularizacijom poprima oblik\n",
    "$$ \n",
    "\\frac{1}{2}\\sum\\limits_{i=1}^m (h_{\\Theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\Theta^T\\Theta.\n",
    "$$\n",
    "\n",
    "Ovakav regularizacijski izraz je u literaturi poznat kao *weight decay*, jer tijekom učenja u nekim iterativnim algoritmima \"*potiče*\" približavanje parametara prema nuli, osim ako podaci ne zahtjevaju drugačije. <br>\n",
    "\n",
    "---\n",
    "Zadatak je sljedeći:\n",
    "- Učitajte podatke 'poly_x.csv' i 'poly_y.csv' i grafički ih prikažite. \n",
    "- Podatke modeliramo funkcijom $h_{\\Theta, \\theta_0}(x) = \\theta_0 + \\Theta_1 x + \\Theta_2 x^2 + \\Theta_3 x^3 + \\Theta_4 x^4 + \\Theta_5 x^5$. \n",
    "- Problem polinomne regresije svodimo na proglem višestruke linearne regresije uvođenjem novih varijabli x_i čija vrijednost će biti x^i. Kreirajte pripadnu matricu dizajna. \n",
    "- Pokrenite gradijentnu metodu s regularizacijom na vašim podacima, pri tome:\n",
    "    - postavite regularizacijski koeficijent na $\\lambda = 0, 1, 5, 20, 100$\n",
    "    - Izračunajte srednje kvadratno odstupanje na skupu za treniranje\n",
    "    - Prikažite graf funkcije $h_{\\Theta, \\theta_0}$\n",
    "    \n",
    "Što možete zaključiti na temelju različitih izbora $\\lambda$? Čemu odgovara slučaj $\\lambda = 0$? \n",
    "\n",
    "Napomena: prilikom treniranja ta tri slučaja, fiksirajte broj iteracija i stopu učenja $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89ca1c5f-47dc-4ed7-b49b-808fcbf3acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_method(X, y, lmbd, lr=0.1, num_iter=50000):\n",
    "    print(f'Starting gradient descent with learning rate {lr}, regularization parameter {lmbd} and {num_iter} iterations')\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(X.shape[1]).reshape(-1, 1)\n",
    "    loss = np.empty(num_iter)\n",
    "    for it in range(num_iter):\n",
    "        loss[it] = (0.5 / m) * np.sum(np.square(X @ theta - y))\n",
    "        # Izracunajte vrijednost funkcije cilja i gradijent\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f33160a-8270-4bb2-a0ad-b913bb50864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(X, y, theta, s):\n",
    "    # X, y podaci\n",
    "    # theta parametar modela\n",
    "    # s stupanj polinoma u model funkciji h_theta\n",
    "    plt.scatter(X, y)\n",
    "    xx = np.linspace(np.min(X), np.max(X), 1000)\n",
    "    XX = np.ones([xx.shape[0],s+1])\n",
    "    for i in range(s):\n",
    "        XX[:,i+1] = xx**(i+1)\n",
    "    yy = np.dot(XX, theta)\n",
    "    plt.plot(xx, yy, 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9155d5-3eb7-4ede-a340-15b7a3956ae5",
   "metadata": {},
   "source": [
    "#### Validacija i stupanj polinoma\n",
    "Primjenom nekog algoritma za odabir optimalnih hiperparametara možemo odrediti optimalni stupanj polinoma unutar model funkcije.\n",
    "\n",
    "Mogli bismo iskoristiti **k-fold unakrsnu validaciju**, natrenirati više modela za svaki stupanj, te odabrati onaj koji nam daje najmanju vrijednost funkcije cilja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aacdc5-6068-43ce-93b6-7f59c62763e9",
   "metadata": {},
   "source": [
    "## Logistička regresija s regularizacijom\n",
    "\n",
    "Regularizirana funkcija cilja u slučaju logističke regresije ima oblik\n",
    "$$ \n",
    "J(\\Theta, \\theta_0) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left[-y^{(i)}\\log{(h_{\\Theta, \\theta_0}(x^{(i)}))}-(1-y^{(i)})\\log{(1-h_{\\Theta, \\theta_0}(x^{(i)}))}\\right] + \\frac{\\lambda}{2m}||\\Theta||^2.\n",
    "$$\n",
    "\n",
    "Zadatak je sljedeći\n",
    "- Učitajte i prikažite podatke iz datoteka 'logi_x.csv' i ' logi_y.csv'. \n",
    "- Podatke separiramo u dvije klase. Kao model funkciju koristit ćemo polinom stupnja 6. Kako ulazni podaci imaju dvije varijable, $x_1$ i $x_2$, to će biti polinom $6$-og stupnja koji sadrži sve monome od $x_1$ i $x_2$, dakle  $h_{\\Theta, \\theta_0}(x) = \\theta_0 + \\Theta_1 x_1 + \\Theta_2 x_2 + \\Theta_3 x_1^2 + \\Theta_4 x_2^2 + \\Theta_5 x_1 x_2 + \\cdots + \\Theta_{26} x_1 x_2^5 + \\Theta_{27} x_2^6$. \n",
    "- Pomoću klase sklearn.preprocessing.PolynomialFeatures napravite odgovarajuću matricu dizajna za vaše podatke\n",
    "- Koristeći implementiranu gradijentnu metodu natrenirajte vaše podatke, kao i u prethodnim zadacima napravite nekoliko slučajeva za $\\lambda = 0, 1, 10, 100$. \n",
    "- Prikažite podatke i pomoću funkcije $\\textrm{plot_boundary}$ prikažite izračunatu granicu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a249c6ec-9e29-4b71-af4b-03772e396a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(X, y, theta, poly_featurizer):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 1000),\n",
    "                         np.linspace(ylim[0], ylim[1], 1000))\n",
    "    \n",
    "    transformed = poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = (np.sign(np.dot(transformed, theta)) + 1) / 2\n",
    "    \n",
    "    # Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce1c9c3e-a8a3-47d7-9ecd-d9ec0ffca14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return (1.0 / (1.0 + np.exp(-z)))\n",
    "\n",
    "def predict(X, theta):\n",
    "    pred = sigma(np.dot(X, theta))\n",
    "    return np.array([1 if p >= 0.5 else 0 for p in pred])\n",
    "\n",
    "def loss_fn(X, y, theta, lmbd):\n",
    "    m = X.shape[0]\n",
    "    h = sigma(X @ theta)\n",
    "    return -(1 / m) * np.sum((y * np.log(h) + (1 - y) * np.log(1 - h))) + (lmbd / (2 * m)) * np.inner(theta[1:], theta[1:])\n",
    "\n",
    "def gradient_method_logreg(X, y, lmbd, lr=0.1, num_iter=50000): \n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    theta =  np.zeros((n,1))\n",
    "    loss = []\n",
    "    j=0\n",
    "    for it in range(num_iter):     \n",
    "        # Izracunajte vrijednost funkcije cilja i gradijent\n",
    "                    \n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46bcca-536b-417b-943c-45a81997c667",
   "metadata": {},
   "source": [
    "## Ugrađena regularizacija u SVM\n",
    "Prilikom rješavanja SVM-a rješavali smo problem uvjetne optimizacije\n",
    "$$\n",
    "\\textrm{argmin}_{\\Theta, \\theta_0, \\xi}\\frac{1}{2}||\\Theta||^2 + C\\sum\\limits_{i=1}^m \\xi_i\n",
    "$$\n",
    "uz uvjet\n",
    "$$\n",
    "\\text{  uz uvjet  } y^{(i)}(\\Theta^T x^{(i)} + \\theta_0) \\geq 1 - \\xi_i \\textrm{ i } \\xi_i \\geq 0, \\text{ }  i = 1, 2, \\dots, m,\n",
    "$$\n",
    "\n",
    "Cilj SVM-a je bio maksimizirati geometrijsku marginu, pa u tom kontekstu $\\frac{1}{2}||\\Theta||^2$ ne promatramo kao regularizacijski parametar, nego *glavni* izraz koji minimiziramo, dok $C\\sum\\limits_{i=1}^m \\xi_i$ igra ulogu regularizacije. Hiperparametar $C$ u ovom slučaju interpretiramo kao koeficijent regularizacije, dok je \"snaga\" regularizacije obrnuto proporcionalna njegovoj vrijednosti.\n",
    "\n",
    "S druge strane, u sličnom (istom do na koeficijente) problemu smo rješavali problem bezuvjetne optimizacije\n",
    "$$\n",
    "\\textrm{argmin}_{\\Theta, \\theta_0}\\lambda||\\Theta||^2 + \\frac{1}{m}\\sum\\limits_{i=1}^m\\ell_{hinge}(y^{(i)}(\\Theta^T x^{(i)} + \\theta_0)).\n",
    "$$\n",
    "U ovom slučaju $\\lambda||\\Theta||^2$ interpretiramo kao regularizacijski izraz, dok je glavni cilj optimizacije minimizirati prosječan hinge gubitak.\n",
    "\n",
    "Zadatak je natrenirati SVM model s različitim regularizacijskim koeficijentima $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec992-51d1-4132-8b28-6acf704227c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e5c07c-3df0-47f8-b4ec-2452ae80a8af",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "Bez korištenja neke vrste validacije teško je odrediti u kojem trenutku treniranje kreće prema overfitanju jer gubitke i metrike računamo samo na skupu za treniranje.\n",
    "\n",
    "Uvođenjem validacijskog skupa nakon svake iteracije optimizacijskog algoritma možemo usporediti gubitak na skupu za treniranje u skupu za validaciju. \n",
    "\n",
    "Ukoliko gubitak na skupu za treniranju pada, a na validacijskom skupu raste ili stagnira, najbolji odabir je ili nekako modificirati optimizacijski algoritam, ili **zaustaviti treniranje**\n",
    "\n",
    "Više o ranom zaustavljanju u domaćoj zadaći."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf6a81-81d5-4e57-a2d7-ef3e7c61d17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
